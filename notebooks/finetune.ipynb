{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16068, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>explicit</th>\n",
       "      <th>song_name</th>\n",
       "      <th>song_popularity</th>\n",
       "      <th>mode</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>release_year</th>\n",
       "      <th>genres</th>\n",
       "      <th>genre</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>song_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3e9HZxeyfWwjeyPAMmWSSQ</th>\n",
       "      <td>Thought I'd end up with Sean. But he wasn't a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>thank u, next</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22900</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>-5.634</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.412</td>\n",
       "      <td>106.966</td>\n",
       "      <td>2019</td>\n",
       "      <td>[dance pop, pop, post-teen pop]</td>\n",
       "      <td>dance/electronic</td>\n",
       "      <td>Ariana Grande</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5p7ujcrUXASCNwRaWNHR1C</th>\n",
       "      <td>Found you when your heart was broke. I filled ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Without Me</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29700</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>-7.050</td>\n",
       "      <td>0.0705</td>\n",
       "      <td>0.533</td>\n",
       "      <td>136.041</td>\n",
       "      <td>2018</td>\n",
       "      <td>[dance pop, electropop, etherpop, indie poptim...</td>\n",
       "      <td>dance/electronic</td>\n",
       "      <td>Halsey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2xLMifQCjDGFmkHkpNLD9h</th>\n",
       "      <td>Astro, yeah. Sun is down, freezin' cold. That'...</td>\n",
       "      <td>1</td>\n",
       "      <td>SICKO MODE</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00513</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>-3.714</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.446</td>\n",
       "      <td>155.008</td>\n",
       "      <td>2018</td>\n",
       "      <td>[pop, pop rap, rap]</td>\n",
       "      <td>pop rap</td>\n",
       "      <td>Travis Scott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1rqqCSm0Qe4I9rUvWncaom</th>\n",
       "      <td>High, high hopes. Had to have high, high hopes...</td>\n",
       "      <td>0</td>\n",
       "      <td>High Hopes</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19300</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>-2.729</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>0.681</td>\n",
       "      <td>82.014</td>\n",
       "      <td>2018</td>\n",
       "      <td>[baroque pop, emo, modern rock, pop punk]</td>\n",
       "      <td>pop</td>\n",
       "      <td>Panic! At The Disco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0bYg9bo50gSsH3LtXe2SQn</th>\n",
       "      <td>I-I-I don't want a lot for Christmas. There is...</td>\n",
       "      <td>0</td>\n",
       "      <td>All I Want for Christmas Is You</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16400</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>-7.462</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.346</td>\n",
       "      <td>150.277</td>\n",
       "      <td>1994</td>\n",
       "      <td>18</td>\n",
       "      <td>christmas</td>\n",
       "      <td>Mariah Carey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   lyrics  \\\n",
       "song_id                                                                     \n",
       "3e9HZxeyfWwjeyPAMmWSSQ  Thought I'd end up with Sean. But he wasn't a ...   \n",
       "5p7ujcrUXASCNwRaWNHR1C  Found you when your heart was broke. I filled ...   \n",
       "2xLMifQCjDGFmkHkpNLD9h  Astro, yeah. Sun is down, freezin' cold. That'...   \n",
       "1rqqCSm0Qe4I9rUvWncaom  High, high hopes. Had to have high, high hopes...   \n",
       "0bYg9bo50gSsH3LtXe2SQn  I-I-I don't want a lot for Christmas. There is...   \n",
       "\n",
       "                        explicit                        song_name  \\\n",
       "song_id                                                             \n",
       "3e9HZxeyfWwjeyPAMmWSSQ         1                    thank u, next   \n",
       "5p7ujcrUXASCNwRaWNHR1C         1                       Without Me   \n",
       "2xLMifQCjDGFmkHkpNLD9h         1                       SICKO MODE   \n",
       "1rqqCSm0Qe4I9rUvWncaom         0                       High Hopes   \n",
       "0bYg9bo50gSsH3LtXe2SQn         0  All I Want for Christmas Is You   \n",
       "\n",
       "                        song_popularity  mode  acousticness  danceability  \\\n",
       "song_id                                                                     \n",
       "3e9HZxeyfWwjeyPAMmWSSQ               86     1       0.22900         0.717   \n",
       "5p7ujcrUXASCNwRaWNHR1C               87     1       0.29700         0.752   \n",
       "2xLMifQCjDGFmkHkpNLD9h               85     1       0.00513         0.834   \n",
       "1rqqCSm0Qe4I9rUvWncaom               86     1       0.19300         0.579   \n",
       "0bYg9bo50gSsH3LtXe2SQn               63     1       0.16400         0.335   \n",
       "\n",
       "                        energy  instrumentalness  liveness  loudness  \\\n",
       "song_id                                                                \n",
       "3e9HZxeyfWwjeyPAMmWSSQ   0.653          0.000000    0.1010    -5.634   \n",
       "5p7ujcrUXASCNwRaWNHR1C   0.488          0.000009    0.0936    -7.050   \n",
       "2xLMifQCjDGFmkHkpNLD9h   0.730          0.000000    0.1240    -3.714   \n",
       "1rqqCSm0Qe4I9rUvWncaom   0.904          0.000000    0.0640    -2.729   \n",
       "0bYg9bo50gSsH3LtXe2SQn   0.625          0.000000    0.0708    -7.462   \n",
       "\n",
       "                        speechiness  valence    tempo  release_year  \\\n",
       "song_id                                                               \n",
       "3e9HZxeyfWwjeyPAMmWSSQ       0.0658    0.412  106.966          2019   \n",
       "5p7ujcrUXASCNwRaWNHR1C       0.0705    0.533  136.041          2018   \n",
       "2xLMifQCjDGFmkHkpNLD9h       0.2220    0.446  155.008          2018   \n",
       "1rqqCSm0Qe4I9rUvWncaom       0.0618    0.681   82.014          2018   \n",
       "0bYg9bo50gSsH3LtXe2SQn       0.0386    0.346  150.277          1994   \n",
       "\n",
       "                                                                   genres  \\\n",
       "song_id                                                                     \n",
       "3e9HZxeyfWwjeyPAMmWSSQ                    [dance pop, pop, post-teen pop]   \n",
       "5p7ujcrUXASCNwRaWNHR1C  [dance pop, electropop, etherpop, indie poptim...   \n",
       "2xLMifQCjDGFmkHkpNLD9h                                [pop, pop rap, rap]   \n",
       "1rqqCSm0Qe4I9rUvWncaom          [baroque pop, emo, modern rock, pop punk]   \n",
       "0bYg9bo50gSsH3LtXe2SQn                                                 18   \n",
       "\n",
       "                                   genre               artist  \n",
       "song_id                                                        \n",
       "3e9HZxeyfWwjeyPAMmWSSQ  dance/electronic        Ariana Grande  \n",
       "5p7ujcrUXASCNwRaWNHR1C  dance/electronic               Halsey  \n",
       "2xLMifQCjDGFmkHkpNLD9h           pop rap         Travis Scott  \n",
       "1rqqCSm0Qe4I9rUvWncaom               pop  Panic! At The Disco  \n",
       "0bYg9bo50gSsH3LtXe2SQn         christmas         Mariah Carey  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_clean = pd.read_pickle(\"df_clean_v1_07122021_py35.pkl\")\n",
    "print(df_clean.shape)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine-tuning tasks: https://ruder.io/recent-advances-lm-fine-tuning/\n",
    "\"The pre-trained model is then fine-tuned on labelled data of a downstream task using a standard cross-entropy loss.\"\n",
    "\n",
    "Hugging Face examples: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imdb example\n",
    "https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/custom_datasets.ipynb#scrollTo=B0WiFsYNDhbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11247, 18)\n",
      "(8034, 18)\n",
      "(8034, 18)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 11\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "train, _eval = train_test_split(df_clean, test_size=val_size + test_size, random_state=seed)\n",
    "val, test = train_test_split(df_clean, test_size=test_size / (val_size + test_size), random_state=seed)\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "# lyrics_valence = DatasetDict()\n",
    "# # arguments expected by forward: https://github.com/huggingface/transformers/blob/9aeacb58bab321bc21c24bbdf7a24efdccb1d426/src/transformers/modeling_bert.py#L1313\n",
    "# lyrics_valence[\"train\"] = Dataset.from_dict({\"text\": X_train, \"labels\": y_train})\n",
    "# lyrics_valence[\"test\"] = Dataset.from_dict({\"text\": X_test, \"labels\": y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare for genre classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dance/electronic', 'jazz/blues/funk', 'disco', 'rock', 'christmas', 'soul/motown', 'pop', 'r&b', 'religious', 'pop rap', 'indie/alternative', 'hard rock/metal', 'acoustic/folk', 'country', 'hip-hop/rap', 'classic rock', 'adult standards', 'reggae', 'latin', 'movie']\n"
     ]
    }
   ],
   "source": [
    "labels_lst = list(set(df_clean[\"genre\"]))\n",
    "print(labels_lst)\n",
    "\n",
    "from datasets import ClassLabel\n",
    "\n",
    "labels = ClassLabel(names=labels_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 11247\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 8034\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 8034\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "lyrics_genre = DatasetDict()\n",
    "\n",
    "# arguments expected by forward: https://github.com/huggingface/transformers/blob/9aeacb58bab321bc21c24bbdf7a24efdccb1d426/src/transformers/modeling_bert.py#L1313\n",
    "lyrics_genre[\"train\"] = Dataset.from_dict({\"text\": list(train[\"lyrics\"]), \"labels\": labels.str2int(train[\"genre\"])})\n",
    "lyrics_genre[\"validation\"] = Dataset.from_dict({\"text\": list(val[\"lyrics\"]), \"labels\": labels.str2int(val[\"genre\"])})\n",
    "lyrics_genre[\"test\"] = Dataset.from_dict({\"text\": list(test[\"lyrics\"]), \"labels\": labels.str2int(test[\"genre\"])})\n",
    "\n",
    "print(lyrics_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8138279c1c5e4bcea34579d696058f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448bdbd360264e48b30618155483e1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bb27bd4b524410b15dcd3331efe761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cast to ClassLabel\n",
    "for _key in lyrics_genre.keys():\n",
    "    new_features = lyrics_genre[_key].features.copy()\n",
    "    new_features[\"labels\"] = labels\n",
    "    lyrics_genre[_key] = lyrics_genre[_key].cast(new_features)\n",
    "    \n",
    "\n",
    "# new_features = lyrics_genre[\"train\"].features.copy()\n",
    "# new_features[\"label\"] = labels\n",
    "\n",
    "# lyrics_genre[\"train\"] = lyrics_genre[\"train\"].cast(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'labels': ClassLabel(num_classes=20, names=['dance/electronic', 'jazz/blues/funk', 'disco', 'rock', 'christmas', 'soul/motown', 'pop', 'r&b', 'religious', 'pop rap', 'indie/alternative', 'hard rock/metal', 'acoustic/folk', 'country', 'hip-hop/rap', 'classic rock', 'adult standards', 'reggae', 'latin', 'movie'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_genre[\"test\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess / tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available models: https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoConfig.from_pretrained\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_str = \"gpt2\"\n",
    "# model_str = \"bert-base-cased\"\n",
    "model_str = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_str, use_fast=True)\n",
    "# tokenizer(lyrics_genre[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa2dc05c75d43b3abd5be8fe985585e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a082e054b949dfa122ef39c4082e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2027282f475643d2a4e8aaf8abe4e683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_str == \"gpt2\":\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # for gpt2\n",
    "\n",
    "max_length = 256\n",
    "    \n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "#     tokens = tokenizer(examples['text'], padding=True, truncation=True, max_length=max_length)\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "tokenized_lyrics = lyrics_genre.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller datasets for iteration\n",
    "n = 1000\n",
    "small_train_dataset = tokenized_lyrics[\"train\"].shuffle(seed=seed).select(range(n)) \n",
    "small_val_dataset = tokenized_lyrics[\"validation\"].shuffle(seed=seed).select(range(n)) \n",
    "small_test_dataset = tokenized_lyrics[\"test\"].shuffle(seed=seed).select(range(n)) \n",
    "\n",
    "full_train_dataset = tokenized_lyrics[\"train\"]\n",
    "full_validation_dataset = tokenized_lyrics[\"validation\"]\n",
    "full_test_dataset = tokenized_lyrics[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels', 'text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(small_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_str, num_labels=len(labels_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "metric_name = \"accuracy\"\n",
    "gpu_batch_size = 0\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_str}-finetuned-genre\", \n",
    "    \n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     per_gpu_train_batch_size=gpu_batch_size,\n",
    "#     per_gpu_eval_batch_size=gpu_batch_size,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "    \n",
    "    evaluation_strategy = \"epoch\",\n",
    "    \n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=1000,\n",
    "    \n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "#     push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DOESN'T WORK!!!\n",
    "# from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TEST = False\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset if TEST else full_train_dataset,\n",
    "    eval_dataset=small_val_dataset if TEST else full_validation_dataset,\n",
    "#     data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['dance/electronic', 'jazz/blues/funk', 'disco', 'rock', 'christmas', 'soul/motown', 'pop', 'r&b', 'religious', 'pop rap', 'indie/alternative', 'hard rock/metal', 'acoustic/folk', 'country', 'hip-hop/rap', 'classic rock', 'adult standards', 'reggae', 'latin', 'movie']\n"
     ]
    }
   ],
   "source": [
    "# check data\n",
    "len(trainer.train_dataset[0][\"attention_mask\"]) == len(trainer.train_dataset[0][\"input_ids\"])\n",
    "print(trainer.train_dataset[0][\"labels\"])\n",
    "print(trainer.train_dataset.features[\"labels\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 11247\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1664' max='1760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1664/1760 18:04 < 01:02, 1.53 it/s, Epoch 4.72/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.117067</td>\n",
       "      <td>0.344785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.296800</td>\n",
       "      <td>1.923089</td>\n",
       "      <td>0.405527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.976700</td>\n",
       "      <td>1.788807</td>\n",
       "      <td>0.452452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.976700</td>\n",
       "      <td>1.681936</td>\n",
       "      <td>0.482823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8034\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-genre/checkpoint-352\n",
      "Configuration saved in distilbert-base-uncased-finetuned-genre/checkpoint-352/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-genre/checkpoint-352/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-genre/checkpoint-352/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-genre/checkpoint-352/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8034\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-genre/checkpoint-704\n",
      "Configuration saved in distilbert-base-uncased-finetuned-genre/checkpoint-704/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-genre/checkpoint-704/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-genre/checkpoint-704/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-genre/checkpoint-704/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8034\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-genre/checkpoint-1056\n",
      "Configuration saved in distilbert-base-uncased-finetuned-genre/checkpoint-1056/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-genre/checkpoint-1056/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-genre/checkpoint-1056/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-genre/checkpoint-1056/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8034\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-genre/checkpoint-1408\n",
      "Configuration saved in distilbert-base-uncased-finetuned-genre/checkpoint-1408/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-genre/checkpoint-1408/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-genre/checkpoint-1408/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-genre/checkpoint-1408/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# debugging: https://huggingface.co/course/chapter8/4?fw=pt\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    del full_train_dataset\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del full_validation_dataset\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del full_test_dataset\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting valence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available models: https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoConfig.from_pretrained\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_str = \"gpt2\"\n",
    "model_str = \"bert-base-cased\"\n",
    "model_str = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "subset = list(df_clean[\"lyrics\"][:n])\n",
    "score = list(df_clean[\"valence\"][:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(subset, score, test_size=0.3, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "lyrics_valence = DatasetDict()\n",
    "# arguments expected by forward: https://github.com/huggingface/transformers/blob/9aeacb58bab321bc21c24bbdf7a24efdccb1d426/src/transformers/modeling_bert.py#L1313\n",
    "lyrics_valence[\"train\"] = Dataset.from_dict({\"text\": X_train, \"labels\": y_train})\n",
    "lyrics_valence[\"test\"] = Dataset.from_dict({\"text\": X_test, \"labels\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787ebc863ff14ca7a972c668613769f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142a234bca354691b9b9edde342ecc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://github.com/huggingface/transformers/issues/2648#issuecomment-616177044\n",
    "if model_str == \"gpt2\":\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # for gpt2\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_lyrics = lyrics_valence.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_lyrics[\"train\"]\n",
    "eval_dataset = tokenized_lyrics[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (700, 4), 'test': (300, 4)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_lyrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'text'],\n",
       "    num_rows: 700\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_lyrics[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_str, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 4\n",
    "gpu_batch_size = 0\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test_trainer\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_gpu_train_batch_size=gpu_batch_size,\n",
    "    per_gpu_eval_batch_size=gpu_batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=eval_dataset, \n",
    "#     compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for cleanup\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 700\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='875' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [875/875 03:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.046868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.042713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.042184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.047476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.047576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to test_trainer/checkpoint-500\n",
      "Configuration saved in test_trainer/checkpoint-500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=875, training_loss=0.037705250331333706, metrics={'train_runtime': 207.4306, 'train_samples_per_second': 16.873, 'train_steps_per_second': 4.218, 'total_flos': 463627627008000.0, 'train_loss': 0.037705250331333706, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "The following columns in the test set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 700\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error 0.06298211452908899\n",
      "train error 0.08451772985381856\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_lyrics[\"test\"])\n",
    "true_val = tokenized_lyrics[\"test\"][\"labels\"]\n",
    "print(\"test error\", np.mean((np.array(predictions[0]) - true_val) ** 2))\n",
    "\n",
    "predictions = trainer.predict(tokenized_lyrics[\"train\"])\n",
    "true_val = tokenized_lyrics[\"train\"][\"labels\"]\n",
    "print(\"train error\", np.mean((np.array(predictions[0]) - true_val) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2509623767202745"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sqrt(0.06298211452908899)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "multi-task with each acoustic feature and genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
