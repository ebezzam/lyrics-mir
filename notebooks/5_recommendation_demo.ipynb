{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo music query\n",
    "\n",
    "We perform the following in order to generate recommendation.\n",
    "\n",
    "1. Load an embeddings and cluster model.\n",
    "2. Query by specifying song title and any metadata to condition.\n",
    "3. Get lyrics through an API.\n",
    "    - First with [this API](http://www.chartlyrics.com/api.aspx), as it's free and does not require an API key.\n",
    "    - Otherwise fall back on [this API](https://github.com/johnwmillr/LyricsGenius) to access Genius. **Note you will need an API key which can create [here](https://genius.com/api-clients).**\n",
    "4. Get Spotify acoustic features and metadata with [this API](https://spotipy.readthedocs.io/en/2.19.0/). **Note you will need a client ID and secret key which can create [here](https://developer.spotify.com).**\n",
    "5. Return top K recommendations by:\n",
    "    - Computing embedding.\n",
    "    - Identifying corresponding cluster.\n",
    "    - Subset based on query.\n",
    "    \n",
    "First some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import lyricsgenius\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pprint import pprint\n",
    "import os\n",
    "import spotipy\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoConfig, AutoModel,AutoModelForPreTraining, AutoTokenizer\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def get_token(token_name, token_path=\"tokens.json\"):\n",
    "    TOKEN = None\n",
    "    if os.environ.get(token_name):\n",
    "        TOKEN = os.environ.get(token_name)\n",
    "    elif os.path.isfile(token_path):\n",
    "        f = open(token_path)\n",
    "        data = json.loads(f.read())\n",
    "        TOKEN = data[token_name]\n",
    "    else:\n",
    "        assert TOKEN is not None, f\"No value for {token_name}.\"\n",
    "    return TOKEN\n",
    "\n",
    "\n",
    "def standardize_lyrics(lyrics, i=0, verbose=False):\n",
    "    if verbose:\n",
    "        print(i)\n",
    "    if lyrics is np.nan or len(lyrics) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # remove new lines\n",
    "    clean = lyrics.replace(\"\\\\n\\\\n\", \". \").replace(\"\\\\n\", \". \").replace(\"\\\\\", \"\")\n",
    "    \n",
    "    # remove square brackets around lyrics\n",
    "    # if possible, extract chorus, pre-chorus, post-chorus, bridge, verses\n",
    "    song_parts = [\"Chorus\", \"Pre-Chorus\", \"Post-Chorus\", \"Bridge\", \"Verse 1\", \"Verse 2\", \"Verse 3\", \"Verse 4\"]\n",
    "    if verbose:\n",
    "        for part in song_parts:\n",
    "            text = find_between(clean, f\"[{part}]. \", \"[\")\n",
    "            if len(text):\n",
    "                print(f\"\\n{part} : {text}\")\n",
    "    \n",
    "    for part in song_parts:\n",
    "        clean = clean.replace(f\"[{part}]. \", \"\")\n",
    "        \n",
    "        \n",
    "    # remove anything else in square brackets\n",
    "    clean = re.sub(\"[\\[].*?[\\]]\", \"\", clean)\n",
    "    \n",
    "    # clean up\n",
    "    clean = clean.replace('\"', \"\")\n",
    "    try:\n",
    "        while clean[0] == \".\" or clean[0] == \" \" or clean[0] == \"'\":\n",
    "            clean = clean[1:]\n",
    "    except:\n",
    "        return np.nan\n",
    "    try:\n",
    "        if clean[-1] == \"'\":\n",
    "            clean = clean[:-1]\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    clean = clean.strip().replace(\"\\n\", \" \")\n",
    "        \n",
    "    return clean\n",
    "\n",
    "\n",
    "def get_embeddings(model_str, lyrics, embedding_fp=None):\n",
    "    \n",
    "    if model_str.endswith('.npy'):\n",
    "        # already computed embeedings\n",
    "        print(\"getting precomputed embeddings\")\n",
    "        corpus_embeddings = np.load(model_str)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # load model\n",
    "        if os.path.isdir(model_str):\n",
    "            config = AutoConfig.from_pretrained(f'{model_str}/config.json')\n",
    "            model = AutoModel.from_config(config)\n",
    "            model = AutoModel.from_pretrained(f'{model_str}/pytorch_model.bin',config=config)\n",
    "            model.eval()\n",
    "            model.cuda()\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_str, use_fast=True)\n",
    "        else:\n",
    "            model = SentenceTransformer(model_str)\n",
    "\n",
    "        # file to save embedding\n",
    "        if embedding_fp is None:\n",
    "            if os.path.isdir(model_str):\n",
    "                embedding_fp = f\"{os.path.split(model_str)[0]}_embeddings.pt\"\n",
    "            else:\n",
    "                embedding_fp = f\"{model_str}_embeddings.pt\"\n",
    "            embedding_fp = os.path.join(\"embeddings\", embedding_fp)\n",
    "            print(embedding_fp)\n",
    "\n",
    "        # load or compute embeddings\n",
    "        if os.path.exists(embedding_fp):\n",
    "            print(\"loading already computed embeddings\")\n",
    "            corpus_embeddings = torch.load(embedding_fp)\n",
    "            if torch.is_tensor(corpus_embeddings):\n",
    "                corpus_embeddings = corpus_embeddings.cpu().data.numpy()\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            if os.path.isdir(model_str):\n",
    "                tokens = tokenizer.batch_encode_plus(\n",
    "                    song_lyrics,\n",
    "                    max_length = 512,\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                )\n",
    "\n",
    "\n",
    "                embed = []\n",
    "                with torch.no_grad():\n",
    "                    for i in tqdm(range(len( df_clean['lyrics']))):\n",
    "                        tkin = tokens['input_ids'][i:i+1]\n",
    "                        tkam = tokens['attention_mask'][i:i+1]\n",
    "\n",
    "                        tkin = torch.tensor(tkin).cuda()\n",
    "                        tkam = torch.tensor(tkam).cuda()\n",
    "\n",
    "                        out = model(tkin,tkam)['last_hidden_state']\n",
    "                        out = out.mean(1).cpu().numpy()\n",
    "\n",
    "                        embed.append(out)\n",
    "                corpus_embeddings = np.vstack(embed)\n",
    "\n",
    "            else:\n",
    "                corpus_embeddings = model.encode(song_lyrics)\n",
    "                corpus_embeddings = corpus_embeddings.cpu().data.numpy()\n",
    "            proc_time = time.time() - start_time\n",
    "            print(f\"Time for computing embeddings : {proc_time} seconds\")\n",
    "            print(f\"{proc_time / num_sentences} seconds per song\")\n",
    "            torch.save(corpus_embeddings, embedding_fp)\n",
    "        \n",
    "    # normalize\n",
    "    embedding_norms = np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "    corpus_embeddings = corpus_embeddings /  embedding_norms\n",
    "\n",
    "\n",
    "    return corpus_embeddings\n",
    "\n",
    "\n",
    "def compute_single_embedding(embeddings_model, lyrics):\n",
    "    if embeddings_model.endswith('.npy'):\n",
    "        # already computed embeedings\n",
    "        corpus_embeddings = np.load(embeddings_model)\n",
    "        song_id = df_query.iloc[0].name\n",
    "        idx = df_clean.index.values == song_id\n",
    "        query_embed = corpus_embeddings[idx][0]\n",
    "    elif not os.path.isdir(embeddings_model):\n",
    "        # coming from model hub\n",
    "        model = SentenceTransformer(embeddings_model)\n",
    "        query_embed = model.encode(lyrics)\n",
    "    else:\n",
    "        # local model\n",
    "        config = AutoConfig.from_pretrained(f'{embeddings_model}/config.json')\n",
    "        model = AutoModel.from_config(config)\n",
    "        model = AutoModel.from_pretrained(f'{embeddings_model}/pytorch_model.bin',config=config)\n",
    "        model.eval()\n",
    "        model.cuda()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(embeddings_model, use_fast=True)\n",
    "\n",
    "        # TODO : simpler for a single lyric?\n",
    "        tokens = tokenizer.batch_encode_plus(\n",
    "            [lyrics],\n",
    "            max_length = 512,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        embed = []\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(len([lyrics]))):\n",
    "                tkin = tokens['input_ids'][i:i+1]\n",
    "                tkam = tokens['attention_mask'][i:i+1]\n",
    "\n",
    "                tkin = torch.tensor(tkin).cuda()\n",
    "                tkam = torch.tensor(tkam).cuda()\n",
    "\n",
    "                out = model(tkin,tkam)['last_hidden_state']\n",
    "                out = out.mean(1).cpu().numpy()\n",
    "\n",
    "                embed.append(out)\n",
    "        query_embed = embed[0][0]\n",
    "\n",
    "    # normalize\n",
    "    query_embed = query_embed /  np.linalg.norm(query_embed)\n",
    "    return query_embed\n",
    "\n",
    "\n",
    "def get_cluster_assignment(model_str, corpus_embeddings, n_cluster, affinity, linkage, clustering_fp=None, print_metrics=False):\n",
    "    \n",
    "    # file to save cluster assignment\n",
    "    if clustering_fp is None:\n",
    "        if os.path.isdir(model_str):\n",
    "            clustering_fp = os.path.split(model_str)[0]\n",
    "        elif model_str.endswith('.npy'):\n",
    "            clustering_fp = os.path.basename(model_str).split(\".\")[0]\n",
    "        else:\n",
    "            clustering_fp = model_str\n",
    "        clustering_fp = clustering_fp + f\"_{n_cluster}clusters_affinity={affinity}_linkage={linkage}\"\n",
    "        clustering_fp = os.path.join(\"clustering\", clustering_fp)\n",
    "        clustering_fp_npz = clustering_fp + \".npz\"\n",
    "        print(clustering_fp_npz)\n",
    "        # backwards compatibility\n",
    "        clustering_fp_npy = clustering_fp + \".npy\"\n",
    "    else:\n",
    "        assert clustering_fp.endswith('.npz')\n",
    "        clustering_fp_npz = clustering_fp\n",
    "    \n",
    "    # load or compute cluster assignment\n",
    "    ch = None\n",
    "    db = None\n",
    "    sh = None\n",
    "            \n",
    "    if os.path.exists(clustering_fp_npz):\n",
    "        print(\"loading already computed cluster assignment\")\n",
    "        data = np.load(clustering_fp_npz)\n",
    "        cluster_assignment = data[\"cluster_assignment\"]\n",
    "        ch = float(data[\"ch\"])\n",
    "        db = float(data[\"db\"])\n",
    "        sh = float(data[\"sh\"])\n",
    "        \n",
    "    elif os.path.exists(clustering_fp_npy):\n",
    "        print(\"loading already computed cluster assignment\")\n",
    "        cluster_assignment = np.load(clustering_fp_npy)\n",
    "        \n",
    "    else:\n",
    "        print(\"start clustering\")\n",
    "        start_time = time.time()\n",
    "        clustering_model = AgglomerativeClustering(n_clusters=n_cluster, affinity=affinity, linkage=linkage, distance_threshold=None)\n",
    "        clustering_model.fit(corpus_embeddings)\n",
    "        cluster_assignment = clustering_model.labels_\n",
    "        proc_time = time.time() - start_time\n",
    "        print(f\"clustering time : {proc_time} seconds\")\n",
    "        \n",
    "    if ch is None:\n",
    "        # compute metrics\n",
    "        print(\"computing metrics\")\n",
    "        ch = calinski_harabasz_score(corpus_embeddings, cluster_assignment)\n",
    "        db = davies_bouldin_score(corpus_embeddings, cluster_assignment)\n",
    "        sh = silhouette_score(corpus_embeddings, cluster_assignment)\n",
    "        \n",
    "        # save everything\n",
    "        print(clustering_fp_npz)\n",
    "        np.savez(\n",
    "            clustering_fp_npz, \n",
    "            cluster_assignment=cluster_assignment, \n",
    "            ch=ch, db=db, sh=sh\n",
    "        )\n",
    "        \n",
    "    if print_metrics:\n",
    "        print(\"\\ncalinski_harabasz_score : \", ch)\n",
    "        print(\"davies_bouldin_score : \", db)\n",
    "        print(\"silhouette_score : \", sh)\n",
    "        \n",
    "    return cluster_assignment, ch, db, sh\n",
    "\n",
    "\n",
    "def subset_conditions(df, embed, cond, song_meta):\n",
    "    embed_sub = embed.copy()\n",
    "    df_sub = df.copy()\n",
    "    for _key in cond:\n",
    "        if cond[_key]:\n",
    "#             print(_key)\n",
    "            if _key == \"genre\":\n",
    "                ind_genre = df_sub[\"genre\"] == cond[_key]\n",
    "                embed_sub = embed_sub[ind_genre]\n",
    "                df_sub = df_sub[ind_genre]\n",
    "            elif _key == \"mode\" or _key == \"explicit\":\n",
    "                if cond[_key] == -1:\n",
    "                    _idx = df_sub[_key] == 0\n",
    "                elif cond[_key] == 1:\n",
    "                    _idx = df_subset[_key] == 1\n",
    "                embed_sub = embed_sub[_idx]\n",
    "                df_sub = df_sub[_idx]\n",
    "            else: \n",
    "                if cond[_key] >= 0:\n",
    "                    _ind = df_sub[_key] > song_meta[_key]\n",
    "                else:\n",
    "                    _ind = df_sub[_key] < song_meta[_key]\n",
    "                embed_sub = embed_sub[_ind]\n",
    "                df_sub = df_sub[_ind]\n",
    "    return df_sub, embed_sub\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) specify query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can write a condition dictionary like below. Don't need all entries if have no preference.\n",
    "\n",
    "conditions = {\n",
    "    \"genre\": None,    # ['dance pop', 'acoustic/folk', 'hip-hop/rap', 'pop', 'soul/disco', 'country', 'r&b', 'rock']\n",
    "    \"mode\": 0,   # -1 for minor, 1 for major, 0 for no preference\n",
    "    \"explicit\": 0,   # -1 for clean, 1 for explicit, 0 for no preference\n",
    "    # positive for more, 0 for no preference, negative for less\n",
    "    'acousticness': 0,\n",
    "    'danceability': 0,\n",
    "    'energy': 0,\n",
    "    'instrumentalness': 0,\n",
    "    'liveness': 0,\n",
    "    'loudness': 0,\n",
    "    'popularity': 0,\n",
    "    'release_year': 0,\n",
    "    'speechiness': 0,\n",
    "    'tempo': 0,\n",
    "    'valence': 0\n",
    "}\n",
    "\n",
    "SimSiam models rely on v3 of the dataset, which have the following genre.\n",
    "Genres for v3: ['rock', 'country', 'dance pop', 'soul', 'acoustic/folk', 'hip-hop/rap', 'r&b', 'alternative/punk', 'disco/house', 'pop', 'adult standards']\n",
    "\n",
    "IF ENCODER IS NOT AVAILABLE YOU CANNOT QUERY OUTSIDE OF DATABASE (e.g. SimSiam).\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" SPECIFY MODEL (from hub or local path) OR PRECOMPUTED EMBEDDINGS \"\"\"\n",
    "# from model hub or local path\n",
    "embeddings_model = \"all-mpnet-base-v2\"\n",
    "embeddings_model = \"all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1735/checkpoint-1735\"\n",
    "# embeddings_model = \"mpnet-genre-valence-finetuned/checkpoint-1041\"\n",
    "# embeddings_model = \"mpnet-genre-valence-energy-finetuned/checkpoint-694\"\n",
    "# embeddings_model = \"mpnet-genre-valence-energy-danceability-finetuned/checkpoint-1041\"\n",
    "# embeddings_model = \"mpnet-genre-valence-energy-danceability-genre-finetuned/checkpoint-1041\"\n",
    "\n",
    "\n",
    "# embeddings_model = \"embeddings/simsiam_mpnet.npy\"    # embeddings nearly identical...\n",
    "# embeddings_model = \"embeddings/simsiam_distilroberta.npy\"   # embeddings nearly identical...\n",
    "\n",
    "\n",
    "# \"\"\" SPECIFY QUERY \"\"\"\n",
    "# # country love song\n",
    "# song_title = \"all i want for christmas is you\"\n",
    "# artist = \"mariah carey\"\n",
    "# conditions = {\n",
    "#     \"genre\": 'country', \n",
    "#     \"valence\": 0\n",
    "# }\n",
    "\n",
    "# # more upbeat, hip-hop song about perserverance\n",
    "# song_title = \"we are the champions\"\n",
    "# artist = \"queen\"\n",
    "# conditions = {\n",
    "#     \"genre\": 'hip-hop/rap', \n",
    "#     \"energy\": 1,\n",
    "#     \"valence\": 1,\n",
    "#     \"danceability\" :1\n",
    "# }\n",
    "\n",
    "\n",
    "# dance pop version of creep that is not explicit\n",
    "song_title = \"This Christmas\"\n",
    "artist = \"Chris Brown\"\n",
    "conditions = {\n",
    "#     \"genre\": 'country',    \n",
    "#     \"explicit\": -1,\n",
    "#     \"valence\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) get lyrics\n",
    "\n",
    "- http://www.chartlyrics.com/api.aspx\n",
    "- https://github.com/johnwmillr/LyricsGenius\n",
    "\n",
    "For last approach, you need an [API token](https://genius.com/api-clients) and add it to your environment variables:\n",
    "```\n",
    "export GENIUS_ACCESS_TOKEN=\"my_access_token_here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENIUS_ACCESS_TOKEN = get_token(\"GENIUS_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found song in database!\n",
      "\n",
      "LYRICS : Hang all the mistletoe. I'm gonna get to know you better. This Christmas. And as we trim the tree. How much fun it's gonna be together. This Christmas. The fireside is blazing bright, woah-oh-oh. We're caroling through the night, woah-oh-oh. This Christmas will be. A very special Christmas for me. Woah-oh-oh-woah-oh-oh-oh. Ha, let's go (Woo!). Presents and cards are here. My world is filled with cheer and you. This Christmas. And as I look around. Your eyes outshine the town, they do. This Christmas. The fireside is blazing bright. And we're caroling through the night. And this Christmas will be. A very special Christmas for me, yeah, huh, woah-oh-oh. Oh, haha, shake a hand, shake a hand now. Na-na-na-na-na-na. . Ooh, the fireside is blazing bright. And we're caroling through the night. And this Christmas will be. So special for you and me, yes, it will be. Woah-oh-oh, ha. Shake a hand now. Come on, everybody shake a hand now, ha. Family, hey. We'll be together. To make this Christmas bright, yes, we will\n"
     ]
    }
   ],
   "source": [
    "lyrics = None\n",
    "song_metadata = None\n",
    "query_embed = None\n",
    "\n",
    "# first query database\n",
    "if embeddings_model == \"embeddings/simsiam_mpnet.npy\" or embeddings_model == \"embeddings/simsiam_distilroberta.npy\":\n",
    "    print(\"using v3 dataset\")\n",
    "    df_clean = pd.read_pickle(\"df_clean_v3_13122021_py35.pkl\")   # for SIAM models\n",
    "    df_songs = np.array([strip_accents(a.lower()) for a in df_clean[\"song_name\"].values])\n",
    "    df_query = df_clean[df_songs == song_title.lower()]\n",
    "else:\n",
    "    df_clean = pd.read_pickle(\"df_clean_v4_14122021_py35.pkl\")\n",
    "    df_songs = np.array([strip_accents(a.lower()) for a in df_clean[\"song_name\"].values])\n",
    "    df_query = df_clean[df_songs == song_title.lower()]\n",
    "if artist is not None:\n",
    "    df_artists = np.array([strip_accents(a.lower()) for a in df_query[\"artist\"].values])\n",
    "    df_query = df_query[df_artists == artist.lower()]\n",
    "if len(df_query) == 1:\n",
    "    print(\"Found song in database!\")\n",
    "    lyrics = df_query[\"lyrics\"].values[0]\n",
    "    song_metadata = df_query.iloc[0].to_dict()\n",
    "    \n",
    "if lyrics is None and embeddings_model.endswith('.npy'):\n",
    "    raise ValueError(\"Didn't find lyrics in database and won't be able to compute embedding without the model (as you've provided precomputed embeddings)\")\n",
    "\n",
    "if lyrics is None:\n",
    "    if artist is not None:\n",
    "        print(\"Trying api.chartlyrics.com\")\n",
    "        start_url = f\"http://api.chartlyrics.com/apiv1.asmx/SearchLyricDirect?artist={strip_accents(artist)}&song={song_title}\"\n",
    "        url = start_url.replace(\" \",\"%20\")\n",
    "        contents = urllib.request.urlopen(url).read()\n",
    "        root = ET.fromstring(contents.decode(\"utf-8\"))\n",
    "        for child in root:\n",
    "            tag = child.tag.split(\"}\")[1]\n",
    "            if tag == \"Lyric\":\n",
    "                lyrics = child.text\n",
    "            if tag == \"LyricSong\":\n",
    "                rec_song = strip_accents(child.text.lower())\n",
    "            if tag == \"LyricArtist\":\n",
    "                rec_artist = strip_accents(child.text.lower())\n",
    "        if rec_song != strip_accents(song_title.lower()) or rec_artist != strip_accents(artist.lower()):\n",
    "            lyrics = None\n",
    "        if lyrics is not None:\n",
    "            lyrics = lyrics.strip().replace(\"\\n\", \" \")\n",
    "\n",
    "    if lyrics is None and GENIUS_ACCESS_TOKEN:\n",
    "        # use Genius API\n",
    "        print(\"Using Genius...\")\n",
    "        genius = lyricsgenius.Genius(GENIUS_ACCESS_TOKEN)\n",
    "        song = genius.search_song(song_title, artist)\n",
    "        lyrics = standardize_lyrics(song.lyrics)\n",
    "        lyrics = ' '.join(lyrics.split(' ')[:-1])[:-13]   # remove last part Genius adds\n",
    "        print(f\"Received song : {song.full_title}\")\n",
    "    #     if song.full_title.lower() != song_title.lower() or song.artist.lower() != artist.lower():\n",
    "    #         lyrics = None\n",
    "    \n",
    "    \n",
    "if lyrics is None:\n",
    "    raise ValueError(\"Could not find song.\")\n",
    "    \n",
    "print(\"\\nLYRICS :\", lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) get spotipy metadata and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "be sure to have credentials from [here](https://developer.spotify.com) and save them as environment variables.\n",
    "```\n",
    "export SPOTIPY_CLIENT_ID='your-spotify-client-id'\n",
    "export SPOTIPY_CLIENT_SECRET='your-spotify-client-secret'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got song metadata from database!\n",
      "\n",
      "{'acousticness': 0.384,\n",
      " 'artist': 'Chris Brown',\n",
      " 'danceability': 0.40700000000000003,\n",
      " 'energy': 0.647,\n",
      " 'explicit': 0,\n",
      " 'genre': 'dance pop',\n",
      " 'instrumentalness': 0.0,\n",
      " 'liveness': 0.0862,\n",
      " 'loudness': -6.031000000000001,\n",
      " 'lyrics': \"Hang all the mistletoe. I'm gonna get to know you better. This \"\n",
      "           \"Christmas. And as we trim the tree. How much fun it's gonna be \"\n",
      "           'together. This Christmas. The fireside is blazing bright, '\n",
      "           \"woah-oh-oh. We're caroling through the night, woah-oh-oh. This \"\n",
      "           'Christmas will be. A very special Christmas for me. '\n",
      "           \"Woah-oh-oh-woah-oh-oh-oh. Ha, let's go (Woo!). Presents and cards \"\n",
      "           'are here. My world is filled with cheer and you. This Christmas. '\n",
      "           'And as I look around. Your eyes outshine the town, they do. This '\n",
      "           \"Christmas. The fireside is blazing bright. And we're caroling \"\n",
      "           'through the night. And this Christmas will be. A very special '\n",
      "           'Christmas for me, yeah, huh, woah-oh-oh. Oh, haha, shake a hand, '\n",
      "           'shake a hand now. Na-na-na-na-na-na. . Ooh, the fireside is '\n",
      "           \"blazing bright. And we're caroling through the night. And this \"\n",
      "           'Christmas will be. So special for you and me, yes, it will be. '\n",
      "           'Woah-oh-oh, ha. Shake a hand now. Come on, everybody shake a hand '\n",
      "           \"now, ha. Family, hey. We'll be together. To make this Christmas \"\n",
      "           'bright, yes, we will',\n",
      " 'mode': 0,\n",
      " 'release_year': 2007,\n",
      " 'song_name': 'This Christmas',\n",
      " 'song_popularity': 39,\n",
      " 'speechiness': 0.125,\n",
      " 'tempo': 88.061,\n",
      " 'valence': 0.39799999999999996}\n"
     ]
    }
   ],
   "source": [
    "if song_metadata is None:\n",
    "    print(\"Getting song metadata from Spotify...\\n\")\n",
    "    SPOTIPY_CLIENT_ID = get_token(\"SPOTIPY_CLIENT_ID\")\n",
    "    SPOTIPY_CLIENT_SECRET = get_token(\"SPOTIPY_CLIENT_SECRET\")\n",
    "\n",
    "    auth_manager = SpotifyClientCredentials(client_id=SPOTIPY_CLIENT_ID, client_secret=SPOTIPY_CLIENT_SECRET)\n",
    "    sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "    # search for song, https://developer.spotify.com/documentation/web-api/reference/#/operations/search\n",
    "    query = f\"track:{song_title}\"\n",
    "    if artist is not None:\n",
    "        query += f\" artist:{artist}\"\n",
    "    res = sp.search(q=query, type='track')\n",
    "\n",
    "    # take top entry\n",
    "    _id = 0\n",
    "    rx_song = res[\"tracks\"][\"items\"][0][\"name\"]\n",
    "    rx_artists = [artist[\"name\"] for artist in res[\"tracks\"][\"items\"][0][\"artists\"]]\n",
    "    print(f\"{rx_song} by {rx_artists}\\n\")\n",
    "\n",
    "    song_metadata = dict()\n",
    "    song_metadata[\"release_year\"] = int(res[\"tracks\"][\"items\"][_id][\"album\"][\"release_date\"][:4])\n",
    "    song_metadata[\"popularity\"] = res[\"tracks\"][\"items\"][_id][\"popularity\"]\n",
    "    song_metadata[\"explicit\"] = res[\"tracks\"][\"items\"][_id][\"explicit\"]\n",
    "\n",
    "    # get acoustic features\n",
    "    acoustic_features = [\"mode\", \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"valence\", \"tempo\"]\n",
    "    uri = res[\"tracks\"][\"items\"][_id][\"uri\"]\n",
    "    feat_results = sp.audio_features(uri)[0]\n",
    "    for _feat in acoustic_features:\n",
    "        song_metadata[_feat] = feat_results[_feat]\n",
    "\n",
    "else:\n",
    "    print(\"Got song metadata from database!\\n\")\n",
    "        \n",
    "pprint(song_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) return top K recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify clustering param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "affinity = \"cosine\"     # “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”. If linkage is “ward”, only “euclidean” is accepted\n",
    "linkage = \"complete\"    # {‘ward’, ‘complete’, ‘average’, ‘single’}, default=’ward’\n",
    "# affinity = \"euclidean\"\n",
    "# linkage = \"ward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first compute embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1735/checkpoint-1735/pytorch_model.bin were not used when initializing MPNetModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing MPNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1735/checkpoint-1735/pytorch_model.bin and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if embeddings_model.endswith('.npy'):\n",
    "    # already computed embeedings\n",
    "    corpus_embeddings = np.load(embeddings_model)\n",
    "    song_id = df_query.iloc[0].name\n",
    "    idx = df_clean.index.values == song_id\n",
    "    query_embed = corpus_embeddings[idx][0]\n",
    "elif not os.path.isdir(embeddings_model):\n",
    "    # coming from model hub\n",
    "    model = SentenceTransformer(embeddings_model)\n",
    "    query_embed = model.encode(lyrics)\n",
    "else:\n",
    "    # local model\n",
    "    config = AutoConfig.from_pretrained(f'{embeddings_model}/config.json')\n",
    "    model = AutoModel.from_config(config)\n",
    "    model = AutoModel.from_pretrained(f'{embeddings_model}/pytorch_model.bin',config=config)\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embeddings_model, use_fast=True)\n",
    "    \n",
    "    # TODO : simpler for a single lyric?\n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        [lyrics],\n",
    "        max_length = 512,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    embed = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len([lyrics]))):\n",
    "            tkin = tokens['input_ids'][i:i+1]\n",
    "            tkam = tokens['attention_mask'][i:i+1]\n",
    "\n",
    "            tkin = torch.tensor(tkin).cuda()\n",
    "            tkam = torch.tensor(tkam).cuda()\n",
    "\n",
    "            out = model(tkin,tkam)['last_hidden_state']\n",
    "            out = out.mean(1).cpu().numpy()\n",
    "\n",
    "            embed.append(out)\n",
    "    query_embed = embed[0][0]\n",
    "\n",
    "# normalize\n",
    "query_embed = query_embed /  np.linalg.norm(query_embed)\n",
    "\n",
    "print(query_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identify corresponding cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering/all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1735_5clusters_affinity=cosine_linkage=complete.npz\n",
      "\n",
      "calinski_harabasz_score :  574.5232967124043\n",
      "davies_bouldin_score :  4.680900109760556\n",
      "silhouette_score :  0.03694266825914383\n",
      "[[0.775772   0.715116   0.49308443 0.9924776  0.7309867 ]]\n",
      "assigned cluster : 2\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(embeddings_model):\n",
    "    clustering_fp = os.path.split(embeddings_model)[0]\n",
    "elif embeddings_model.endswith('.npy'):\n",
    "    clustering_fp = os.path.basename(embeddings_model).split(\".\")[0]\n",
    "else:\n",
    "    clustering_fp = embeddings_model\n",
    "    \n",
    "# TODO : could be npz file with scores...\n",
    "clustering_fp += f\"_{n_clusters}clusters_affinity={affinity}_linkage={linkage}.npz\"\n",
    "clustering_fp = os.path.join(\"clustering\", clustering_fp)\n",
    "print(clustering_fp)\n",
    "\n",
    "# check if clustering already exists\n",
    "song_lyrics = df_clean['lyrics'].tolist()\n",
    "if os.path.isfile(clustering_fp):\n",
    "    data = np.load(clustering_fp)\n",
    "    cluster_assignment = data[\"cluster_assignment\"]\n",
    "    ch = float(data[\"ch\"])\n",
    "    db = float(data[\"db\"])\n",
    "    sh = float(data[\"sh\"])\n",
    "    \n",
    "else:\n",
    "    # compute with clustering notebook\n",
    "    print(\"Cluster assignment not available. Computing...\")\n",
    "    embeddings = get_embeddings(embeddings_model, song_lyrics)\n",
    "    cluster_assignment, ch, db, sh = get_cluster_assignment(\n",
    "        embeddings_model, embeddings, n_clusters, affinity, linkage\n",
    "    )\n",
    "    \n",
    "print(\"\\ncalinski_harabasz_score : \", ch)\n",
    "print(\"davies_bouldin_score : \", db)\n",
    "print(\"silhouette_score : \", sh)\n",
    "    \n",
    "    \n",
    "# compute centroids\n",
    "# -- load embeddings\n",
    "# corpus_embeddings = get_embeddings(embeddings_model, song_lyrics)\n",
    "\n",
    "if embeddings_model.endswith('.npy'):\n",
    "    # already computed embeedings\n",
    "    corpus_embeddings = np.load(embeddings_model)\n",
    "else:\n",
    "        \n",
    "    if os.path.isdir(embeddings_model):\n",
    "        embedding_fp = f\"{os.path.split(embeddings_model)[0]}_embeddings.pt\"\n",
    "    else:\n",
    "        embedding_fp = f\"{embeddings_model}_embeddings.pt\"\n",
    "    embedding_fp = os.path.join(\"embeddings\", embedding_fp)\n",
    "    assert os.path.isfile(embedding_fp)\n",
    "    corpus_embeddings = torch.load(embedding_fp)\n",
    "    if torch.is_tensor(corpus_embeddings):\n",
    "        corpus_embeddings = corpus_embeddings.cpu().data.numpy()\n",
    "\n",
    "assert len(corpus_embeddings) == len(cluster_assignment)\n",
    "\n",
    "# -- average according to cluster assignment\n",
    "centroids = []\n",
    "for i in range(n_clusters):\n",
    "    inds = cluster_assignment == i\n",
    "    centroids.append(np.mean(corpus_embeddings[inds,:], axis=0))\n",
    "centroids = np.vstack(centroids)\n",
    "\n",
    "# identify closest cluster according to correct metric\n",
    "query_embed = query_embed / np.linalg.norm(query_embed, axis=-1, keepdims=True)\n",
    "\n",
    "if affinity == \"cosine\":\n",
    "    scores = cosine_distances(query_embed[np.newaxis, :], centroids)\n",
    "elif affinity == \"euclidean\":\n",
    "    scores = euclidean_distances(query_embed[np.newaxis, :], centroids)\n",
    "else:\n",
    "    raise ValueError\n",
    "assigned_cluster = np.argmin(scores)\n",
    "print(scores)\n",
    "print(\"assigned cluster :\", assigned_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset based on query and give top K recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs in cluster : 1453\n",
      "Songs after conditioning : 1453\n"
     ]
    }
   ],
   "source": [
    "# subset according to cluster\n",
    "inds = cluster_assignment == assigned_cluster\n",
    "embeddings_subset = corpus_embeddings[inds]\n",
    "df_subset = df_clean.iloc[inds]\n",
    "print(\"Songs in cluster :\", len(embeddings_subset))\n",
    "\n",
    "# subset according to conditions\n",
    "df_subset, embeddings_subset = subset_conditions(df_subset, embeddings_subset, conditions, song_metadata)\n",
    "\n",
    "print(\"Songs after conditioning :\", len(embeddings_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.42766857\n",
      "Genre: soul/disco Artist: Larry Graham SongName: When We Get Married\n",
      "Lyrics: Darling, oh.... When we get married. We'll have a big celebration. And send invitations. To all our friends and relations. And we'll have a ball. Dancin' and all. When we get married. When the bells ring. To tell the world I'm taking your hand. Folks from all over. Will come to see the wedding we pl\n",
      "*****\n",
      "Score: 0.44080472\n",
      "Genre: pop Artist: Barry Manilow SongName: It's a Miracle\n",
      "Lyrics: You wouldn't believe where I've been. The cities and towns I've been in. From Boston to Denver. And every town in between. (Everyone looks the same). The people they all look the same. (Yes, the same). Oh, only the names have been changed. (Just the names). But now that I'm home again. I'll tell you\n",
      "*****\n",
      "Score: 0.4504236\n",
      "Genre: soul/disco Artist: Natalie Cole SongName: Party Lights\n",
      "Lyrics: I see the party lights. Shining in the night. Make me feel all right. I see the party lights. Shining in the night. Really out of sight. I see reflections going in the air. Telling me the way to move. I feel the spirit and it's everywhere. Telling me it's time to groove. I see the party lights. Shin\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "# compute scores\n",
    "if affinity == \"cosine\":\n",
    "    scores = cosine_distances(query_embed[np.newaxis, :], embeddings_subset)[0]\n",
    "elif affinity == \"euclidean\":\n",
    "    scores = euclidean_distances(query_embed[np.newaxis, :], embeddings_subset)[0]\n",
    "\n",
    "K = 3\n",
    "max_len = 300  # for printing lyrics\n",
    "\n",
    "topk = scores.argsort()[:K]\n",
    "for i in topk:\n",
    "    print(\"Score:\", scores[i])\n",
    "    print('Genre:',df_subset['genre'][i],'Artist:',df_subset['artist'][i],'SongName:',df_subset['song_name'][i])\n",
    "    print(\"Lyrics:\", df_subset['lyrics'][i][:max_len])\n",
    "    print('*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recommendations without clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before condition subsetting :  15863\n",
      "After condition subsetting :  15863\n",
      "\n",
      "Score: 0.07271087\n",
      "Genre: dance pop Artist: Chris Brown SongName: This Christmas\n",
      "Lyrics: Hang all the mistletoe. I'm gonna get to know you better. This Christmas. And as we trim the tree. How much fun it's gonna be together. This Christmas. The fireside is blazing bright, woah-oh-oh. We're caroling through the night, woah-oh-oh. This Christmas will be. A very special Christmas for me. W\n",
      "*****\n",
      "Score: 0.42766857\n",
      "Genre: soul/disco Artist: Larry Graham SongName: When We Get Married\n",
      "Lyrics: Darling, oh.... When we get married. We'll have a big celebration. And send invitations. To all our friends and relations. And we'll have a ball. Dancin' and all. When we get married. When the bells ring. To tell the world I'm taking your hand. Folks from all over. Will come to see the wedding we pl\n",
      "*****\n",
      "Score: 0.44080472\n",
      "Genre: pop Artist: Barry Manilow SongName: It's a Miracle\n",
      "Lyrics: You wouldn't believe where I've been. The cities and towns I've been in. From Boston to Denver. And every town in between. (Everyone looks the same). The people they all look the same. (Yes, the same). Oh, only the names have been changed. (Just the names). But now that I'm home again. I'll tell you\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "# subset according to conditions\n",
    "print(\"Before condition subsetting : \", len(corpus_embeddings))\n",
    "df_subset, embeddings_subset_no_clust = subset_conditions(df_clean, corpus_embeddings, conditions, song_metadata)\n",
    "print(\"After condition subsetting : \", len(embeddings_subset_no_clust))\n",
    "print()\n",
    "\n",
    "# compute scores\n",
    "if affinity == \"cosine\":\n",
    "    scores_no_clust = cosine_distances(query_embed[np.newaxis, :], embeddings_subset_no_clust)[0]\n",
    "elif affinity == \"euclidean\":\n",
    "    scores_no_clust = euclidean_distances(query_embed[np.newaxis, :], embeddings_subset_no_clust)[0]\n",
    "\n",
    "topk = scores_no_clust.argsort()[:K]\n",
    "for i in topk:\n",
    "    print(\"Score:\", scores_no_clust[i])\n",
    "    print('Genre:',df_subset['genre'][i],'Artist:',df_subset['artist'][i],'SongName:',df_subset['song_name'][i])\n",
    "    print(\"Lyrics:\", df_subset['lyrics'][i][:max_len])\n",
    "    print('*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recommendation for base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings/all-mpnet-base-v2_embeddings.pt\n",
      "loading already computed embeddings\n",
      "Before condition subsetting :  15863\n",
      "After condition subsetting :  15863\n",
      "\n",
      "\n",
      "Score: 0.0\n",
      "Genre: dance pop Artist: Chris Brown SongName: This Christmas\n",
      "Lyrics: Hang all the mistletoe. I'm gonna get to know you better. This Christmas. And as we trim the tree. How much fun it's gonna be together. This Christmas. The fireside is blazing bright, woah-oh-oh. We're caroling through the night, woah-oh-oh. This Christmas will be. A very special Christmas for me. W\n",
      "*****\n",
      "Score: 0.23767686\n",
      "Genre: pop Artist: Andy Williams SongName: It's the Most Wonderful Time of the Year\n",
      "Lyrics: It's the most wonderful time of the year. With the kids jingle belling. And everyone telling you be of good cheer. It's the most wonderful time of the year. It's the hap-happiest season of all. With those holiday greetings. And gay happy meetings when friends come to call. It's the hap-happiest seas\n",
      "*****\n",
      "Score: 0.24062991\n",
      "Genre: pop Artist: Justin Bieber SongName: Mistletoe\n",
      "Lyrics: It’s the most beautiful time of the year. Lights fill the streets, spreading so much cheer. I should be playing in the winter snow. But I'ma be under the mistletoe. I don’t wanna miss out on the holiday. But I can’t stop staring at your face. I should be playing in the winter snow. But I'ma be under\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "base_embeddings = get_embeddings(\"all-mpnet-base-v2\", song_lyrics)\n",
    "query_emd_base = compute_single_embedding(\"all-mpnet-base-v2\", lyrics)\n",
    "\n",
    "\n",
    "# subset according to conditions\n",
    "print(\"Before condition subsetting : \", len(base_embeddings))\n",
    "df_subset, base_embeddings_subset = subset_conditions(df_clean, base_embeddings, conditions, song_metadata)\n",
    "print(\"After condition subsetting : \", len(base_embeddings_subset))\n",
    "print()\n",
    "\n",
    "\n",
    "# compute scores\n",
    "if affinity == \"cosine\":\n",
    "    scores_base = cosine_distances(query_emd_base[np.newaxis, :], base_embeddings_subset)[0]\n",
    "elif affinity == \"euclidean\":\n",
    "    scores_base = euclidean_distances(query_emd_base[np.newaxis, :], base_embeddings_subset)[0]\n",
    "\n",
    "print()\n",
    "topk = scores_base.argsort()[:K]\n",
    "for i in topk:\n",
    "    print(\"Score:\", scores_base[i])\n",
    "    print('Genre:',df_subset['genre'][i],'Artist:',df_subset['artist'][i],'SongName:',df_subset['song_name'][i])\n",
    "    print(\"Lyrics:\", df_subset['lyrics'][i][:max_len])\n",
    "    print('*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
