{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 13 09:25:12 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   35C    P0    57W / 300W |  28612MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel,AutoModelForPreTraining, AutoTokenizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# df_clean = pd.read_pickle(\"df_clean_v1_07122021_py35.pkl\")\n",
    "df_clean = pd.read_pickle(\"df_clean_v4_14122021_py35.pkl\")\n",
    "print(df_clean.shape)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = \"all-mpnet-base-v2\"\n",
    "model_str = \"all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1735/checkpoint-1735\"\n",
    "model_str = \"all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1041/checkpoint-1041\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1041/checkpoint-1041/pytorch_model.bin were not used when initializing MPNetModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing MPNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1041/checkpoint-1041/pytorch_model.bin and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(model_str):\n",
    "    config = AutoConfig.from_pretrained(f'{model_str}/config.json')\n",
    "    model = AutoModel.from_config(config)\n",
    "    model = AutoModel.from_pretrained(f'{model_str}/pytorch_model.bin',config=config)\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_str, use_fast=True)\n",
    "else:\n",
    "    model = SentenceTransformer(model_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings/all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1041_embeddings.pt\n",
      "loading already computed embeddings\n",
      "(15863, 768)\n"
     ]
    }
   ],
   "source": [
    "song_lyrics = df_clean['lyrics'].tolist()\n",
    "song_names = df_clean['song_name'].tolist()\n",
    "song_artists = df_clean['artist'].tolist()\n",
    "song_genres = df_clean['genre'].tolist()\n",
    "num_sentences = len(song_lyrics)\n",
    "\n",
    "\n",
    "if os.path.isdir(model_str):\n",
    "    embedding_fp = f\"{os.path.split(model_str)[0]}_embeddings.pt\"\n",
    "else:\n",
    "    embedding_fp = f\"{model_str}_embeddings.pt\"\n",
    "embedding_fp = os.path.join(\"embeddings\", embedding_fp)\n",
    "print(embedding_fp)\n",
    "\n",
    "if os.path.exists(embedding_fp):\n",
    "    print(\"loading already computed embeddings\")\n",
    "    corpus_embeddings = torch.load(embedding_fp)\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    if os.path.isdir(model_str):\n",
    "        tokens = tokenizer.batch_encode_plus(\n",
    "            song_lyrics,\n",
    "            max_length = 512,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        embed = []\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(len( df_clean['lyrics']))):\n",
    "                tkin = tokens['input_ids'][i:i+1]\n",
    "                tkam = tokens['attention_mask'][i:i+1]\n",
    "\n",
    "                tkin = torch.tensor(tkin).cuda()\n",
    "                tkam = torch.tensor(tkam).cuda()\n",
    "\n",
    "                out = model(tkin,tkam)['last_hidden_state']\n",
    "                out = out.mean(1).cpu().numpy()\n",
    "\n",
    "                embed.append(out)\n",
    "        corpus_embeddings = np.vstack(embed)\n",
    "        \n",
    "    else:\n",
    "        corpus_embeddings = model.encode(song_lyrics)\n",
    "        corpus_embeddings = corpus_embeddings.cpu().data.numpy()\n",
    "    proc_time = time.time() - start_time\n",
    "    print(f\"Time for computing embeddings : {proc_time} seconds\")\n",
    "    print(f\"{proc_time / num_sentences} seconds per song\")\n",
    "    torch.save(corpus_embeddings, embedding_fp)\n",
    "    \n",
    "    \n",
    "print(corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering/all-mpnet-base-v2-finetuned-genre_unfrozen_base-checkpoint-1041_11clusters_affinity=cosine_linkage=complete.npy\n",
      "loading already computed cluster assignment\n",
      "calinski_harabasz_score :  366.4661612437651\n",
      "davies_bouldin_score 4.587638110067143\n",
      "silhouette_score 0.03756221\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 11\n",
    "affinity = \"cosine\"   # “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”. If linkage is “ward”, only “euclidean” is accepted\n",
    "linkage = \"complete\"   # {‘ward’, ‘complete’, ‘average’, ‘single’}, default=’ward’\n",
    "\n",
    "# normalize\n",
    "embedding_norms = np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "corpus_embeddings = corpus_embeddings /  embedding_norms\n",
    "\n",
    "if os.path.isdir(model_str):\n",
    "    clustering_fp = os.path.split(model_str)[0]\n",
    "else:\n",
    "    clustering_fp = model_str\n",
    "clustering_fp += f\"_{n_clusters}clusters_affinity={affinity}_linkage={linkage}.npy\"\n",
    "clustering_fp = os.path.join(\"clustering\", clustering_fp)\n",
    "print(clustering_fp)\n",
    "\n",
    "if os.path.exists(clustering_fp):\n",
    "    print(\"loading already computed cluster assignment\")\n",
    "    cluster_assignment = np.load(clustering_fp)\n",
    "else:\n",
    "    print(\"start clustering\")\n",
    "    start_time = time.time()\n",
    "    clustering_model = AgglomerativeClustering(n_clusters=n_clusters, affinity=affinity, linkage=linkage, distance_threshold=None)\n",
    "    clustering_model.fit(corpus_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    proc_time = time.time() - start_time\n",
    "    print(f\"clustering time : {proc_time} seconds\")\n",
    "    np.save(clustering_fp, cluster_assignment)\n",
    "    \n",
    "# print metrics\n",
    "score = calinski_harabasz_score(corpus_embeddings, cluster_assignment)\n",
    "print(\"calinski_harabasz_score : \", score)\n",
    "\n",
    "score = davies_bouldin_score(corpus_embeddings, cluster_assignment)\n",
    "print(\"davies_bouldin_score : \", score)\n",
    "\n",
    "score = silhouette_score(corpus_embeddings, cluster_assignment)\n",
    "print(\"silhouette_score : \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sweep parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(X=corpus_embeddings)\n",
    "print(pca_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "cm = plt.get_cmap('gist_rainbow')\n",
    "# clrs = [cm(i//3*3.0/n_clusters) for i in range(n_clusters)]\n",
    "clrs = sns.color_palette('husl', n_colors=n_clusters)  # a list of RGB tuples\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "start_idx = 0\n",
    "for i in range(n_clusters):\n",
    "    cluster_mask = cluster_assignment == i\n",
    "    pca_cluster = pca_result[cluster_mask]\n",
    "    ax.scatter3D(pca_cluster[:, 0], pca_cluster[:, 1], pca_cluster[:, 2], color=clrs[i], label=i)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
