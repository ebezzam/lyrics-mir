{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast, RobertaTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_pickle(\"df_clean_v3_13122021_py35.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrs_data(genre,attributes):\n",
    "    all_lyrs=[]\n",
    "    all_attrs=torch.tensor([])\n",
    "    genre_labels = []\n",
    "    for i,g in enumerate(genre):\n",
    "        lyrsd = df_clean[df_clean[\"genre\"] == g]\n",
    "        lyrs = list(lyrsd[\"lyrics\"][:].values)\n",
    "        genre_labels += [i]*len(lyrs)\n",
    "        all_lyrs += lyrs\n",
    "        \n",
    "        attrs = []\n",
    "        for a in attributes:\n",
    "            attrs.append(lyrsd[a].to_list())\n",
    "        attrs = torch.cat([torch.tensor(t).reshape(-1,1) for t in attrs],1)\n",
    "        all_attrs = torch.cat([all_attrs,attrs],0)\n",
    "    return all_lyrs,all_attrs,genre_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitlyrs_data(genre,attributes):\n",
    "    all_first_lyrs=[]\n",
    "    all_sec_lyrs=[]\n",
    "    all_attrs=torch.tensor([])\n",
    "    genre_labels = []\n",
    "    for i,g in enumerate(genre):\n",
    "        lyrsd = df_clean[df_clean[\"genre\"] == g]\n",
    "        lyrs = list(lyrsd[\"lyrics\"][:].values)\n",
    "        genre_labels += [i]*len(lyrs)\n",
    "        for lyr in lyrs:\n",
    "            lyrlen = len(lyr)//2\n",
    "            first_half = lyr[:lyrlen]\n",
    "            sec_half = lyr[lyrlen:]\n",
    "            \n",
    "            all_first_lyrs += [first_half]\n",
    "            all_sec_lyrs += [sec_half]\n",
    "        \n",
    "        attrs = []\n",
    "        for a in attributes:\n",
    "            attrs.append(lyrsd[a].to_list())\n",
    "        attrs = torch.cat([torch.tensor(t).reshape(-1,1) for t in attrs],1)\n",
    "        all_attrs = torch.cat([all_attrs,attrs],0)\n",
    "    return all_first_lyrs,all_sec_lyrs,all_attrs,genre_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15916 15916 torch.Size([15916, 1])\n"
     ]
    }
   ],
   "source": [
    "genres = df_clean[\"genre\"].unique()\n",
    "# attr = ['danceability','energy','valence']\n",
    "attr = ['valence']\n",
    "Xf,Xs,y,g_l = get_splitlyrs_data(genres,attr)\n",
    "print(len(Xf),len(Xs),y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thought I'd end up with Sean. But he wasn't a match. Wrote some songs about Ricky. Now I listen and laugh. Even almost got married. And for Pete, I'm so thankful. Wish I could say, Thank you to Malcolm. 'Cause he was an angel. One taught me love. One taught me patience. And one taught me pain. Now, I'm so amazing. Say I've loved and I've lost. But that's not what I see. So, look what I got. Look what you taught me. And for that, I say. Thank you, next (Next). Thank you, next (Next). Thank you, next. I'm so fuckin' grateful for my ex. Thank you, next (Next). Thank you, next (Next). Thank you, next (Next). I'm so fuckin'—. Spend more time with my friends. I ain't worried 'bout nothin'. Plus, I met someone else. We havin' better discussions. I know they say I move on too fast. But this one gon' last. 'Cause her name is Ari. And I'm so good with that (So good with that). She taught me love (Love). She taught me patience (Patience). How she handles pain (Pain). That shit's amazing (Yeah, she's amazing). I've loved and I've lost (Yeah, yeah). But that's not what I see (Yeah, yeah). 'Cause look what I've found (Yeah, yeah). Ain't no need for searching, and for that, I say. Thank you, next (Thank you, next). Thank you, nex\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"t (Thank you, next). Thank you, next (Thank you). I'm so fuckin' grateful for my ex. Thank you, next (Thank you, next). Thank you, next (Said thank you, next). Thank you, next (Next). I'm so fuckin' grateful for my ex. Thank you, next. Thank you, next. Thank you, next. I'm so fuckin'—. One day I'll walk down the aisle. Holding hands with my mama. I'll be thanking my dad. 'Cause she grew from the drama. Only wanna do it once, real bad. Gon' make that shit last. God forbid something happens. Least this song is a smash (Song is a smash). I've got so much love (Love). Got so much patience (Patience). I've learned from the pain (Pain). I turned out amazing (Turned out amazing). Say I've loved and I've lost (Yeah, yeah). But that's not what I see (Yeah, yeah). 'Cause look what I've found (Yeah, yeah). Ain't no need for searching. And for that, I say. Thank you, next (Thank you, next). Thank you, next (Thank you, next). Thank you, next. I'm so fuckin' grateful for my ex. Thank you, next (Thank you, next). Thank you, next (Said thank you, next). Thank you, next (Next). I'm so fuckin' grateful for my ex. Thank you, next. Thank you, next. Thank you, next. Yeah, yee. Thank you, next. Thank you, next. Thank you, next. Yeah, yee\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelBase(nn.Module):\n",
    "    def __init__(self,base_model):\n",
    "        super(BertModelBase,self).__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "#         self.freeze_params(self.base_model)\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        \n",
    "      \n",
    "    def freeze_params(self,net):\n",
    "        for p in net.parameters():\n",
    "            p.require_grads = False\n",
    "    \n",
    "    def D(self,p, z, version='simplified'):\n",
    "\n",
    "        return - F.cosine_similarity(p, z.detach(), dim=-1).mean()\n",
    "\n",
    "    def forward(self,sent_id1,mask1,sent_id2,mask2):\n",
    "        baseout1 = self.base_model(sent_id1, attention_mask=mask1)['pooler_output']\n",
    "        baseout2 = self.base_model(sent_id2, attention_mask=mask2)['pooler_output']\n",
    "\n",
    "        z1 = self.fc1(baseout1)\n",
    "        z1 = self.relu(z1)\n",
    "\n",
    "        z2 = self.fc1(baseout2)\n",
    "        z2 = self.relu(z2)\n",
    "        \n",
    "\n",
    "        # output layer\n",
    "        p1 = self.fc2(z1)\n",
    "        p1 = self.relu(p1)\n",
    "        \n",
    "        p2 = self.fc2(z2)\n",
    "        p2 = self.relu(p2)\n",
    "        \n",
    "        L = self.D(p1, z2) / 2 + self.D(p2, z1) / 2\n",
    "\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "# model = BertModelBase(bert)\n",
    "# model = BertModelAttributes(mpnet_base,1)\n",
    "# tokenizer = model.tokenizer\n",
    "\n",
    "bert = AutoModel.from_pretrained('sentence-transformers/all-distilroberta-v1')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sentence-transformers/all-distilroberta-v1')\n",
    "model = BertModelBase(bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train_firsthalf = tokenizer.batch_encode_plus(\n",
    "    Xf,\n",
    "    max_length = 512,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "tokens_train_sechalf = tokenizer.batch_encode_plus(\n",
    "    Xs,\n",
    "    max_length = 512,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_first = torch.tensor(tokens_train_firsthalf['input_ids'])\n",
    "train_mask_first = torch.tensor(tokens_train_firsthalf['attention_mask'])\n",
    "\n",
    "train_seq_sec = torch.tensor(tokens_train_sechalf['input_ids'])\n",
    "train_mask_sec = torch.tensor(tokens_train_sechalf['attention_mask'])\n",
    "\n",
    "train_y = y\n",
    "# train_y = torch.tensor(g_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq_first, train_mask_first, train_seq_sec,train_mask_sec)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "notrain_dataloader = DataLoader(train_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-3)          # learning rate\n",
    "\n",
    "celoss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Train epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    all_loss = []\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to('cuda') for r in batch]\n",
    "\n",
    "        sent_id1, mask1, sent_id2, mask2 = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        loss = model(sent_id1, mask1,sent_id2, mask2 )\n",
    "#         print(preds, labels)\n",
    "\n",
    "\n",
    "        all_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    return all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 995/995 [08:01<00:00,  2.07it/s]\n",
      "100%|██████████| 995/995 [08:01<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "lossoverepoch = []\n",
    "for i in range(2):\n",
    "    lossoverepoch += train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9f12491198>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ll=[]\n",
    "for l in lossoverepoch:\n",
    "    if l<1:\n",
    "        ll.append(l)\n",
    "plt.plot(ll)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
